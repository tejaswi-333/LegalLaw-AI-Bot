{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8b99863",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Downgrade langchain to the last stable version (v0.3) to support RetrievalQA\n",
    "%pip install \"langchain<1.0.0\" \"langchain-community<1.0.0\" \"langchain-core<1.0.0\" \"langchain-openai<1.0.0\" \"langchain-text-splitters<1.0.0\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa77e202",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------------------------------------------------------------\n",
    "# CELL 1: SETUP, IMPORTS, AND CONFIGURATION\n",
    "# ------------------------------------------------------------------\n",
    "\n",
    "import os\n",
    "import time\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Import necessary LangChain components\n",
    "from langchain_pinecone import PineconeVectorStore\n",
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain.chains import RetrievalQA\n",
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "# 1. Load API Keys from .env file\n",
    "load_dotenv()\n",
    "\n",
    "# --- CONFIGURATION ---\n",
    "# Must match the Index Name used in Milestone 1\n",
    "INDEX_NAME = \"legal-assistant\"\n",
    "# Must match the Embedding Model used in Milestone 1\n",
    "EMBEDDING_MODEL = \"all-MiniLM-L6-v2\" \n",
    "\n",
    "# Check for keys\n",
    "if not os.environ.get(\"OPENAI_API_KEY\"):\n",
    "    print(\"‚ùå Error: OPENAI_API_KEY not found in .env file.\")\n",
    "else:\n",
    "    print(\"‚úÖ API Keys loaded successfully.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cad1f949",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------------------------------------------------------------\n",
    "# CELL 2: CONNECT TO PINECONE DATABASE\n",
    "# ------------------------------------------------------------------\n",
    "\n",
    "print(f\"‚è≥ Loading embedding model: {EMBEDDING_MODEL}...\")\n",
    "# We use the same model as M1 to ensure vectors match\n",
    "embeddings = HuggingFaceEmbeddings(model_name=EMBEDDING_MODEL)\n",
    "\n",
    "print(f\"‚è≥ Connecting to Pinecone Index: {INDEX_NAME}...\")\n",
    "vectorstore = PineconeVectorStore(\n",
    "    index_name=INDEX_NAME, \n",
    "    embedding=embeddings\n",
    ")\n",
    "\n",
    "print(\"‚úÖ Successfully connected to Pinecone VectorStore.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a37e3cf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------------------------------------------------------------\n",
    "# CELL 3: DATABASE VERIFICATION (Human Check)\n",
    "# ------------------------------------------------------------------\n",
    "test_query = \"fundamental rights\"\n",
    "print(f\"üîé Running Test Query: '{test_query}'\")\n",
    "\n",
    "try:\n",
    "    # We fetch just 1 document to prove the 'ingestion' worked\n",
    "    raw_results = vectorstore.similarity_search(test_query, k=1)\n",
    "    \n",
    "    if raw_results:\n",
    "        print(\"‚úÖ Database Check Passed!\")\n",
    "        doc = raw_results[0]\n",
    "        print(f\"   ‚Ä¢ Found Document: {doc.metadata.get('document_title', 'Untitled')}\")\n",
    "        print(f\"   ‚Ä¢ Source File:    {doc.metadata.get('source_file')}\")\n",
    "        print(f\"   ‚Ä¢ Content Snippet: {doc.page_content[:100]}...\")\n",
    "    else:\n",
    "        print(\"‚ùå Database Check Failed. No results found.\")\n",
    "        print(\"   STOP: Please check if Milestone 1 ingestion was successful.\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Connection Error: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0361b4ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------------------------------------------------------------\n",
    "# CELL 4: BUILD LEGABOT RAG CHAIN\n",
    "# ------------------------------------------------------------------\n",
    "\n",
    "# 1. Setup Retriever (The Tool for the AI)\n",
    "# We convert the vectorstore into a retriever interface\n",
    "retriever = vectorstore.as_retriever(\n",
    "    search_type=\"similarity\",\n",
    "    search_kwargs={\"k\": 5} # Retrieve top 5 most relevant chunks\n",
    ")\n",
    "\n",
    "# 2. Initialize OpenAI (The Brain)\n",
    "print(\"‚è≥ Initializing ChatOpenAI (gpt-3.5-turbo)...\")\n",
    "llm = ChatOpenAI(\n",
    "    model_name=\"gpt-3.5-turbo\", \n",
    "    temperature=0.1 # Low temperature = Precise, Factual Legal Answers\n",
    ")\n",
    "\n",
    "# 3. Define the System Prompt (Based on Project Doc Section 5)\n",
    "legal_system_prompt = \"\"\"\n",
    "You are LegaBot, a precise legal research assistant. \n",
    "For any user query, use the following context to provide an answer.\n",
    "\n",
    "GUIDELINES:\n",
    "1. Retrieve and list the most relevant statute sections/judgment excerpts.\n",
    "2. Provide a concise legal summary (3‚Äì5 sentences).\n",
    "3. If the answer requires legal interpretation or could affect rights, clearly say you are not a lawyer and recommend consulting a qualified attorney.\n",
    "4. Always prioritize citing the exact statutory language or judgment excerpt used.\n",
    "5. If you don't know the answer based on the context, say \"I don't have enough information in my legal database.\"\n",
    "\n",
    "CONTEXT:\n",
    "{context}\n",
    "\n",
    "QUESTION:\n",
    "{question}\n",
    "\n",
    "YOUR ANSWER:\n",
    "\"\"\"\n",
    "\n",
    "PROMPT = PromptTemplate(\n",
    "    template=legal_system_prompt, \n",
    "    input_variables=[\"context\", \"question\"]\n",
    ")\n",
    "\n",
    "# 4. Construct the Chain\n",
    "# Connects: Retriever -> Prompt -> LLM\n",
    "qa_chain = RetrievalQA.from_chain_type(\n",
    "    llm=llm,\n",
    "    chain_type=\"stuff\",\n",
    "    retriever=retriever,\n",
    "    return_source_documents=True, # Required to show sources\n",
    "    chain_type_kwargs={\"prompt\": PROMPT}\n",
    ")\n",
    "\n",
    "print(\"‚úÖ LegaBot Chain built successfully. Ready to answer.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "405ed70d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------------------------------------------------------------\n",
    "# CELL 5: RUN LEGABOT (With Detailed Top 5 Text Answers)\n",
    "# ------------------------------------------------------------------\n",
    "\n",
    "# Define your question\n",
    "user_query = \"What is the punishment for theft under Indian law?\"\n",
    "\n",
    "print(f\"‚ùì USER QUESTION: '{user_query}'\")\n",
    "print(\"ü§ñ LEGABOT IS THINKING...\")\n",
    "\n",
    "# Run the chain\n",
    "start_time = time.time()\n",
    "response = qa_chain.invoke({\"query\": user_query})\n",
    "end_time = time.time()\n",
    "\n",
    "# --- 1. DISPLAY THE AI'S MAIN SUMMARY ---\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(f\"üìù LEGABOT FINAL ANSWER ({end_time - start_time:.2f}s):\")\n",
    "print(\"=\"*60)\n",
    "print(response[\"result\"].strip())\n",
    "\n",
    "# --- 2. DISPLAY THE TOP 5 RETRIEVED TEXTS (THE \"5 RESPONSES\") ---\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"üîç TOP 5 RETRIEVED SOURCES (The Exact Text Found):\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Loop through the source docs and print the ACTUAL TEXT content\n",
    "for i, doc in enumerate(response[\"source_documents\"]):\n",
    "    title = doc.metadata.get('document_title', 'Unknown Title')\n",
    "    source = doc.metadata.get('source_file', 'Unknown File')\n",
    "    \n",
    "    print(f\"\\n[{i+1}] SOURCE: {title}\")\n",
    "    print(f\"    File: {source}\")\n",
    "    # ‚¨áÔ∏è THIS IS THE NEW PART: Printing the actual text content ‚¨áÔ∏è\n",
    "    clean_text = \" \".join(doc.page_content.split())\n",
    "    print(f\"    üìñ EXCERPT: \\\"{clean_text}...\\\"\")\n",
    "    print(\"-\" * 60)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
